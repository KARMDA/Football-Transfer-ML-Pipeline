# -*- coding: utf-8 -*-
"""3 model building

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nwSGNpmLyJg7zkcyqjFpAR0vrtNc21X7
"""

import pandas as pd

df = pd.read_csv("Final_Merge.csv")

print(df.columns.tolist())

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("Final_Merge.csv")

# Target
TARGET = "market_value_millions"

# Drop non-feature columns
DROP_COLS = [
    "player_name", "team", "sentiment_classification",
    "market_value_last_updated"
]

X = df.drop(columns=[TARGET] + DROP_COLS, errors="ignore")
y = df[TARGET]

# One-hot encode categorical columns
X = pd.get_dummies(X, drop_first=True)

# Fill missing values
X = X.fillna(X.median())

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("Training samples:", X_train.shape)
print("Testing samples:", X_test.shape)

# ================================
# MODEL 1: RANDOM FOREST REGRESSOR
# Target: market_value_millions
# ================================

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ------------------------------------------------
# 1. Load Dataset
# ------------------------------------------------
df = pd.read_csv("/content/Final_Merge.csv")
print("Dataset shape:", df.shape)

# ------------------------------------------------
# 2. Define Target and Features
# ------------------------------------------------
target = "market_value_millions"

features = [
    "matches_played",
    "minutes_played",
    "passes",
    "shots",
    "goals",
    "assists",
    "tackles",
    "interceptions",
    "yellow_cards",
    "assists_per_match",
    "goals_per_match",
    "minutes_per_match",
    "defensive_index",
    "discipline_index",
    "ball_possession_index",
    "performance_score",
    "injury_risk_score",
    "total_injuries_history",
    "days_injured_last_season",
    "avg_recovery_time_days",
    "days_since_last_injury",
    "twitter_mentions_count",
    "social_sentiment_score",
    "public_perception_index"
]

X = df[features]
y = df[target]

# ------------------------------------------------
# 3. Handle Missing Values
# ------------------------------------------------
X = X.fillna(X.median())
y = y.fillna(y.median())

# ------------------------------------------------
# 4. Train-Test Split (80-20)
# ------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ------------------------------------------------
# 5. Train Random Forest Model
# ------------------------------------------------
rf_model = RandomForestRegressor(
    n_estimators=300,
    max_depth=12,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# ------------------------------------------------
# 6. Predictions
# ------------------------------------------------
y_pred = rf_model.predict(X_test)

# ------------------------------------------------
# 7. Evaluation Metrics
# ------------------------------------------------
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n RANDOM FOREST RESULTS")
print("MAE :", mae)
print("RMSE:", rmse)
print("R²  :", r2)

# ------------------------------------------------
# 8. Feature Importance (Top 10)
# ------------------------------------------------
feature_importance = pd.DataFrame({
    "feature": features,
    "importance": rf_model.feature_importances_
}).sort_values(by="importance", ascending=False)

print("\n Top 10 Important Features")
print(feature_importance.head(10))

# =========================
# XGBOOST REGRESSION MODEL
# =========================

# Install xgboost if not present
!pip install xgboost

import pandas as pd
import numpy as np

from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------------------------
# 1. Load Dataset
# -------------------------
df = pd.read_csv("Final_Merge.csv")

# -------------------------
# 2. Define Target & Features
# -------------------------
TARGET = "market_value_millions"

DROP_COLS = [
    "player_name",
    "team",
    "sentiment_classification",
    "market_value_last_updated"
]

X = df.drop(columns=[TARGET] + DROP_COLS, errors="ignore")
y = df[TARGET]

# -------------------------
# 3. Encoding & Cleaning
# -------------------------
X = pd.get_dummies(X, drop_first=True)
X = X.fillna(X.median())

# -------------------------
# 4. Train-Test Split
# -------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# -------------------------
# 5. Scaling (Numerical Only)
# -------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -------------------------
# 6. Train XGBoost Model
# -------------------------
xgb_model = XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    random_state=42
)

xgb_model.fit(X_train, y_train)

# -------------------------
# 7. Predictions
# -------------------------
y_pred = xgb_model.predict(X_test)

# -------------------------
# 8. Evaluation Metrics
# -------------------------
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(" XGBOOST RESULTS")
print("MAE :", mae)
print("RMSE:", rmse)
print("R²  :", r2)

# -------------------------
# 9. Feature Importance
# -------------------------
feature_importance = pd.DataFrame({
    "feature": X.columns,
    "importance": xgb_model.feature_importances_
}).sort_values(by="importance", ascending=False)

print("\n Top 10 Important Features (XGBoost)")
print(feature_importance.head(10))

# =========================
# LIGHTGBM REGRESSION MODEL
# =========================

# Install LightGBM
!pip install lightgbm

import pandas as pd
import numpy as np

from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


# 1. Load Dataset

df = pd.read_csv("Final_Merge.csv")

# 2. Define Target & Features

TARGET = "market_value_millions"

DROP_COLS = [
    "player_name",
    "team",
    "sentiment_classification",
    "market_value_last_updated"
]

X = df.drop(columns=[TARGET] + DROP_COLS, errors="ignore")
y = df[TARGET]

# 3. Encoding & Cleaning

X = pd.get_dummies(X, drop_first=True)
X = X.fillna(X.median())

# 4. Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# 5. Scaling (Numerical Only)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# 6. Train LightGBM Model

lgbm_model = LGBMRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=-1,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

lgbm_model.fit(X_train, y_train)

# -------------------------
# 7. Predictions
# -------------------------
y_pred = lgbm_model.predict(X_test)

# -------------------------
# 8. Evaluation Metrics
# -------------------------
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# MAE (Mean Absolute Error) - Average absolute prediction error
# RMSE (Root Mean Squared Error) - Penalizes large errors
# R² Score -  Measures variance explained by model

print("LIGHTGBM RESULTS")
print("MAE :", mae)
print("RMSE:", rmse)
print("R²  :", r2)

# -------------------------
# 9. Feature Importance
# -------------------------
feature_importance = pd.DataFrame({
    "feature": X.columns,
    "importance": lgbm_model.feature_importances_
}).sort_values(by="importance", ascending=False)

print("\n Top 10 Important Features (LightGBM)")
print(feature_importance.head(10))